{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Natural Language Toolkit (NLTK)\n",
    "## An interactive tutorial for beginners\n",
    "\n",
    "*(UIUC MCSDS CS410 Technology Review Project by John Moran and Graham Chester)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "John Moran - jfmoran2: Division of Labor, 50% share: jointly reviewed existing tutorials for NLTK, Stopwords and Tokenizers: jointly worked on text and coding, Stemming and Lemmatization: text and coding, Parts of Speech Tagging: text and coding.\n",
    "\n",
    "Graham Chester - grahamc2: Division of Labor, 50% share: jointly reviewed existing tutorials for NLTK, Downloading NLTK Data: text and coding, Basic NLTK Operations section: text and coding,  Stopwords and Tokenizers: jointly worked on text and coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our CS410 technical review for the Natural Language Toolkit (NLTK).  Instead of just writing a PDF paper evaluating NLTK, we decided it would be useful if we went further, and not only explained and reviewed NLTK, but also developed an in-depth working tutorial in a Jupyter notebook for students interested in getting started with NLTK.\n",
    "\n",
    "NLTK is a platform for writing Python programs for natural language processing (NLP) applications.  This is a tutorial on some of the basic NLTK functionality and is meant as an introduction for beginners. However, the tutorial assumes you are familiar with programming in Python.  If not, basic Python tutorials and installation instructions can be found here: \n",
    "\n",
    "https://www.python.org/about/gettingstarted/\n",
    "\n",
    "#### Version Notes:\n",
    "\n",
    "- This tutorial was written in **Python 3.6**\n",
    "\n",
    "- For the latest instructions on downloading and installing **NLTK version 3.4**, please see the official website: http://www.nltk.org/install.html.   \n",
    "\n",
    "- This tutorial accesses features that contained bugs in **NLTK v3.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will cover the following topics:\n",
    "* Installation\n",
    "* Downloading NLTK Data\n",
    "* Downloading NLTK Data\n",
    "* Basic NLTK Operations\n",
    "* Stopwords and Tokenizers\n",
    "* Stemming and Lemmatization\n",
    "* Parts of Speech Tagging\n",
    "* Further Reading on NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Option 1: Cloud\n",
    "\n",
    "There are two options for installation. The first is not actually an installation, no files or data will be written to your local machine. This Jupyter notebook can be started directly from [mybinder](https://mybinder.org/v2/gh/jfmoran2/CS410_Public_TechReview/master). It will take several minutes to start as it copies across file from this github [repo](https://github.com/jfmoran2/CS410_Public_TechReview), then builds and starts a docker container with the Python required libraries.\n",
    "\n",
    "### Option 2: Local Machine\n",
    "\n",
    "The easiest way to install the prerequisites, if they are not already on your Windows, Mac or Linux machine, is to download and install Anaconda (Python version 3) from [here](https://www.anaconda.com/download), and then \"conda install nltk\", or refer to the official [NLTK website](http://www.nltk.org/install.html)\n",
    "\n",
    "If you have an existing Python 3.5 or above installation and don't wish to install Anaconda, you can do the following, but you may need to be careful with versions:\n",
    "\n",
    "```Python\n",
    "pip install matplotlib jupyter nltk\n",
    "```\n",
    "\n",
    "Next, clone or download the GitHub repo from [here](https://github.com/jfmoran2/CS410_Public_TechReview). This contains the Jupyter notebook, **NLTK_Tutorial.ipynb**.\n",
    "\n",
    "At a terminal/command line window you then type 'jupyter notebook' in the directory that contains the notebook. This will start the notebook server at port 8888 on your local machine and open a browser window. If you have any problems check this [quickstart guide](https://jupyter.readthedocs.io/en/latest/content-quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading NLTK Data\n",
    "\n",
    "NLTK provides dozens of optional models, packages, grammars, and corpora (i.e. text collections) that you may choose to install.  \n",
    "\n",
    "To get started downloading the NLTK data, first import the nltk module:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the NLTK packages, pass a parameter to the download command specifying which package you want to download. For the purposes of this tutorial, you will only need to download the eight packages below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('masc_tagged')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If you are running on your LOCAL machine and would like to download *all* the packages, you may specify the parameter, **all**.  Uncomment and run the following command if you would like to download all packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: only uncomment the following command if you have 3+ GB available in local disk storage!\n",
    "#nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLTK Operations\n",
    "\n",
    "The first collection we downloaded above, *gutenberg* is a small selection of texts from Project Gutenberg, which is an electronic archive project that has tens of thousands of books online.  The *gutenberg* corpus is defined as *Plaintext Corpora* in NLTK, which is exactly as it sounds, non-tagged plain text.  \n",
    "\n",
    "The second collection we downloaded above, *masc_tagged* is a tagged corpus from the *Manually Annotated Sub-Corpus (MASC)* project.  MASC is a subset of hundreds of thousands of English words from written works and transcribed speech.  The MASC corpus defined as *Tagged Corpora* in NLTK and is annotated with different tokenizations, parts of speech tags, noun/verb shallow parsing, etc. \n",
    "\n",
    "There are many other corpus formats and corpus readers classes defined in the package *nltk.corpus*.  For more detailed information on the available NLTK corpora and corpus reader classes see:   http://www.nltk.org/howto/corpus.html#corpus-reader-classes\n",
    "\n",
    "To see the texts available from the *gutenberg* corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg_fileids = gutenberg.fileids()\n",
    "print(gutenberg_fileids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a list of the names of the books we can now access. We will select 'Alice in Wonderland' and open it with the NLTK plain text corpus reader class method **words()**. This reader class provides different data access methods to access the data in the *gutenberg* corpus, such as:\n",
    "- words()\n",
    "- sents()\n",
    "- paras()\n",
    "- raw()\n",
    "\n",
    "For detailed information on the NLTK corpus reader classes, see: http://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing and counting words\n",
    "Let's first access the words in 'Alice in Wonderland':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of words for 'Alice in Wonderland'\n",
    "alice_words = gutenberg.words(\"carroll-alice.txt\")\n",
    "alice_nwords = len(alice_words)\n",
    "print(alice_nwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the first and last word of the collection of words, simply use the appropriate index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (alice_words[0])\n",
    "print (alice_words[alice_nwords - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our concept of \"word\" might be different than NLTK's.  A word here can be any sequence of non-space characters, or a punctuation symbol.  Let's look at the first 20 words\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alice_words[1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of unique keywords in the word collection, use the **set** command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(alice_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the count of unique words, use the **len** command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_unique_word_count = len(set(alice_words))\n",
    "print (initial_unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique words here is overestimated because we haven't considered upper and lower case.  Let's see if the number changes when we force all the text to lowercase and then count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words_LC = [word.lower() for word in alice_words]\n",
    "actual_unique_word_count = len(set(alice_words_LC))\n",
    "print(actual_unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to count the number of specific word occurences, you can use the **count** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words.count('Alice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to parsing the text into words, the plain text corpus reader class also provides a method for accessing the sentences in the text, using the **sent** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_sentences = gutenberg.sents(\"carroll-alice.txt\")\n",
    "\n",
    "sentence_count = len(alice_sentences)\n",
    "print(\"Number of sentences: {}\".format(sentence_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any given sentence in the text, just index the array. For example, to access the 105th sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alice_sentences[104])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of utility of the **sent** function, we could calculate the average number of words in each sentence: (keeping in mind this includes punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of words per sentence: {}\".format(alice_nwords/sentence_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching text\n",
    "If you want to see the specific occurences within the text, along with some context in which the word appears, you can you the **concordance()** function, but first we must import the NLTK **Text** package and convert our words into an NLTK **Text** object. Note that **concordance()** is not case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text  \n",
    "textList = Text(alice_words)\n",
    "textList.concordance(\"drink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try **concordance()** with 'Alice' as the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textList.concordance('Alice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will only return the top 25 matches.  If you want to see all the matches, you have to use the **lines** parameter:\n",
    "\n",
    "Note that the **lines** parameter function did not work for values larger than the default size of 25 in NLTK v3.3 and was fixed in v3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textList.concordance('Alice', lines=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a plot of the word offset certain words occur in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "textList.dispersion_plot([\"Alice\",\"drink\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ConcordanceIndex** class is closely related to the **Concordance** class and provides additional functionality for obtaining the indices of the the search word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import ConcordanceIndex\n",
    "conIndex = ConcordanceIndex(alice_words)\n",
    "conIndex.print_concordance(\"drink\")\n",
    "print(conIndex.offsets('drink'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency\n",
    "To access the word frequency data in the text, use the **FreqDist** class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = nltk.FreqDist(alice_words_LC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to find the frequent words in a text is to call the **most_common** function and pass in N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to find a specific word in the frequency distribution, just pass it as an text index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rabbit appears {} times\".format(fdist['rabbit']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Access some more **FreqDist** methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The word appearing the most times is {}\".format(fdist.max()))\n",
    "print(\"The number of words in the distribution is {}\".format(fdist.N()))\n",
    "print(\"The number of unique words in the distribution is {}\".format(fdist.B()))\n",
    "print(\"The frequency of the word \\'alice\\' is {}\".format(fdist.freq('alice')))\n",
    "print(\"The frequency of the word \\'drink\\' is {}\".format(fdist.freq('drink')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the cumulative word distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords and Tokenizers\n",
    "\n",
    "Stopwords are high frequency, common words, such as \"a\", \"an\", \"the\", \"from\", \"to\", etc. Most of the time, NLP applications want to filter out (i.e. remove), stopwords before doing lexical/syntactic analysis because they have no added value to the analysis and can contribute to misleading results.  For example, if you are comparing the similarity of two documents and don't remove stopwords, the similarity results will be misleading, showing a higher similarity simply due to matching common stopwords.\n",
    "\n",
    "### Stopwords\n",
    "NLTK has a corpus of 2,400 stopwords in eleven language built into it.  To include it and load the English language stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "print (english_stopwords)\n",
    "print(\"\\nNumber of English stopwords is {}\".format(len(english_stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration of language differences, German has 30% more stopwords than English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stopwords = stopwords.words('german')\n",
    "print(german_stopwords)\n",
    "print(\"\\nNumber of German stopwords is {}\".format(len(german_stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the English stopwords from our lower case word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words_new = [word for word in alice_words_LC if word not in english_stopwords]\n",
    "\n",
    "print(\"Number of words in \\'Alice in Wonderland\\' = {}\".format(len(alice_words_LC)))\n",
    "print(\"Number of words remaining after stopwords removed = {}\".format(len(alice_words_new)))\n",
    "print(\"Percentage of the text that consisted of stopwords = {0:.2f}%\".format(100*((len(alice_words_LC)-len(alice_words_new))/len(alice_words_LC))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(alice_words_new)\n",
    "print (fdist)\n",
    "fdist.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after the stopwords are removed, many of the top words are now punctuation symbols.  We could use Python to parse off words consisting solely of non-alphabetic characters and rebuild our list, but a quick and dirty way of keeping only \"interesting\" words, is to simply discard any word less than 3 characters and then look at the frequency distribution again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words_new_truncated = [word for word in alice_words_new if word  if len(word) > 2 ]\n",
    "\n",
    "fdist = nltk.FreqDist(alice_words_new_truncated)\n",
    "fdist.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on stopwords, see https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides numerous powerful tokenizers that can parse text using different algorithms dependent upon your specific application needs.  For a full list and explaination of each NLTK tokenizer, see:  http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "For a more in-depth look at the tokenization process, see: \n",
    "https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en\n",
    "\n",
    "For a detailed look at the complications of multi-word tokenization (such as \"hot dog\" and \"red tape\"), see this research paper: https://d-nb.info/1046313010/34\n",
    "\n",
    "For a comparison of NLTK to other open source tokenizers, see this paper: https://d-nb.info/1046313010/34\n",
    "\n",
    "In this example, we show three tokenizers--\n",
    "\n",
    "- **RegexpTokenizer**: a regular expression tokenizer\n",
    "- **TweetTokenizer**: a tokenizer specifically designed to handle tweets from Twitter\n",
    "- **word_tokenizer**: NLTK's recommended tokenizer for basic textual parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "regexpTokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "text = \"I have a secret.  I can't tell you.  #keepingQuiet\"\n",
    "\n",
    "tweetTokens = tweetTokenizer.tokenize(text)\n",
    "regexpTokens = regexpTokenizer.tokenize(text)\n",
    "wordTokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tweet tokens  {}\".format(tweetTokens))\n",
    "print(\"Regexp tokens {}\".format(regexpTokens))\n",
    "print(\"Word tokens   {}\".format(wordTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output is different for each tokenizer.  \n",
    "\n",
    "The **TweetTokenizer** leaves punctuation intact, as in the word *can't*, but recognizes the text *#keepingQuiet* as a Twitter hashtag, so does not delete the *#* nor separate it from the following text.\n",
    "\n",
    "The **RegexpTokenizer** uses the regular expression **\\w+**, which greedily matches alphanumeric characters up until it encounters a non-alphanumeric character.  Notice that it deletes not only the *.* and the *#*, but also deletes the apostrophe in *can't*, leaving the tokens *can* and *t*.  For a good summary of regular expressions, see: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285\n",
    "\n",
    "The **word_tokenize** tokenizer leaves the punctuation between words intact, and splits the word *can't* into two tokens, *ca* and *n't*.  This tokenizer is NLTK's recommended tokenizer and is actually a combination of two other basic NLTK tokenizers, first applying the NLTK **PunktSentenceTokenizer** followed by the NLTK **TreebankWordTokenizer**.  It applies more intelligence to the sentence and word parsing than a simple regular expression tokenizer is able to.\n",
    "\n",
    "When we used the corpus reader method **alice_words = gutenberg.words(\"carroll-alice.txt\")** earlier to parse the 'Alice in Wonderland' text, the **words()** method actually is a wrapper that calls this **word_tokenize** function.\n",
    "\n",
    "To illustrate the differences between using this methodology versus a basic regular expression parser, we can read in the 'Alice' text using the **raw** function, parse it with the regular expression parser, convert to lowercase, remove stopwords and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_raw = gutenberg.raw(\"carroll-alice.txt\")\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "alice_toks = tokenizer.tokenize(alice_raw)\n",
    "alice_toks_LC = [word.lower() for word in alice_toks]\n",
    "alice_toks_new = [word for word in alice_toks_LC if word not in english_stopwords]\n",
    "\n",
    "print(\"Number of unique words in our initial analysis: {}\".format(len(set(alice_words_new_truncated))))\n",
    "print(\"Number of unique words using reg exp tokenizer: {}\".format(len(set(alice_toks_new))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a slight difference in the number of words.  Let's compare the top five words from each tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_orig = nltk.FreqDist(alice_words_new_truncated)\n",
    "fdist_regexp = nltk.FreqDist(alice_toks_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Orig   {}\".format(fdist_orig.most_common(5)))\n",
    "print(\"Regex  {}\".format(fdist_regexp.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the results were the same in our current application, but bear in mind for other applications, this might not be the case.  After we discuss parts of speech tagging later in this tutorial, we will show how using these two different tokenizers can result in very different results.  This point is discussed in the final note of this tutorial titled: *Tokenizer/POS caution*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming is used to reduce words to their \"stems\", i.e. their root form, by removing/replacing suffixes. For example, reducing the words: *eating, eaten, eats* to their common root *eat*. \n",
    "\n",
    "Lemmatization is a process of determining the lemma, or base form of the word.  Whereas stemming can be cruder with suffixes simply removed leaving a stem that isn't a valid word (such as *neurology* being reduced to the stem *neurolog*), lemmatization is a more complex process where a dictionary is accesssed and grammatical rules applied to determine a valid base form for the word.\n",
    "\n",
    "In NLTK, there are two stemmers, the Porter Stemmer and the Lancaster Stemmer.  The Porter Stemmer is based on algorithm developed in 1980.  The original paper can be found here: https://tartarus.org/martin/PorterStemmer/def.txt\n",
    "\n",
    "The Lancaster algorithm was developed more recently, and in general is considered more \"aggressive\" than the Porter algorithm in terms of reducing words to shorter stems.\n",
    "\n",
    "NLTK uses the WordNet Lemmatizer.  WordNet is a large lexical database of English language words that can be found here: https://wordnet.princeton.edu/  \n",
    "\n",
    "For a more detailed examination of stemming algorithms, see: http://research.ijais.org/volume4/number3/ijais12-450655.pdf\n",
    "and http://www.informationr.net/ir/19-1/paper605.html#.XAoO_BMzai4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"automation\",\"automate\",\"automates\"]\n",
    "[porter.stem(w) for w in list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lancaster.stem(w) for w in list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemmatizer.lemmatize(w) for w in list1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the stemmers both produced the same result, while the lemmatizer left the words intact.  \n",
    "\n",
    "It is easy to create another simple example where the stemmers differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = [\"runs\",\"running\",\"runner\"]\n",
    "[porter.stem(w) for w in list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lancaster.stem(w) for w in list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemmatizer.lemmatize(w) for w in list2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the porter stemmer retained the word \"runner\", whereas the lancaster stemmer shortened it to the root \"run\".\n",
    "\n",
    "The lemmatizer kept the word list intact, except it dropped the \"s\" in runs.\n",
    "\n",
    "Similarly, the stemmers and lemmatizer produce different results when we apply the algorithms to our Alice in Wonderland token set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_porter = [porter.stem(word) for word in sorted(set(alice_toks_new))]\n",
    "alice_lancaster = [lancaster.stem(word) for word in sorted(set(alice_toks_new))]\n",
    "alice_lemma = [lemmatizer.lemmatize(word) for word in sorted(set(alice_toks_new))]\n",
    "\n",
    "print(alice_lemma[1:10])\n",
    "print(alice_porter[1:10])\n",
    "print(alice_lancaster[1:10])\n",
    "\n",
    "print(\"Original count of unique words: {}\".format(len(set(alice_toks_new))))\n",
    "print(\"Lemmatization stem count: {}\".format(len(set(alice_lemma))))\n",
    "print(\"Porter stem count: {}\".format(len(set(alice_porter))))\n",
    "print(\"Lancaster stem count: {}\".format(len(set(alice_lancaster))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first 10 words in the set are stemmed slightly differently and that the total number of unique stems differs, with the lancaster stemmer producing the fewest stems, as might be expected from both the algorithm description of being the most aggressive stemmer, and from observing our simple \"runner\" example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common task in NLP is classifying words into their parts of speech (POS) (e.g. verbs, nouns, adjectives, etc.).  This categorization process is commonly referred to as POS-tagging.\n",
    "\n",
    "For a more detailed look at POS tagging in NLP, see: https://pdfs.semanticscholar.org/4119/324f5bdbbf6b620e90ea26f5e4d27e6b8de0.pdf and https://nlp.stanford.edu/pubs/CICLing2011-manning-tagging.pdf\n",
    "\n",
    "NLTK provides functionality to tag plain text in addition to providing corpora that is already pre-tagged.  In the first step of this tutorial we downloaded the MASC tagged corpora, which we will look at in this section.\n",
    "\n",
    "Below is a simple example of POS tagging starting with plain text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "example_text = word_tokenize(\"This is a sample sentence that has a few words in it\")\n",
    "nltk.pos_tag(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the **pos_tag** function is a list of (word, POS) tuples.  In the above example, the tuple **('is', 'VBZ')** means that the word 'is' is a verb, present tense, 3rd person singular.  The default tagset abbreviations can be non-intuitive, before you become familiar with the tagset.\n",
    "\n",
    "For an explanation of each of the tagged parts of speech, issue the following NLTK help command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of scrolling through the above list of POS tags, you can look up a specific POS tag by passing the tag abbreviation in as a parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"NNP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your plain text is already tokenized, you can simply call the **pos_tag** function directly.  For example, we can tag our original text of 'Alice in Wonderland' with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(alice_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK tagging algorithms are not perfect, and obviously can make mistakes with ambiguous words, etc, which is why pre-tagged corpora can be useful to NLP projects.  For example, the MASC tagged corpus is manually verfied for validity so it can be used as a baseline for testing POS NLP software.\n",
    "\n",
    "To see all of the pre-tagged texts available in the MASC corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import masc_tagged\n",
    "\n",
    "masc_fileids = masc_tagged.fileids()\n",
    "print(masc_fileids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will select the first text, which is a tagged transcript of the second U.S. Presidential debate between Al Gore and George W. Bush which took place in October, 2000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gore_bush_words = masc_tagged.words('spoken/2nd_Gore-Bush.txt')\n",
    "\n",
    "print(\"Transcript length: {}\".format(len(gore_bush_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the method **tagged_words()** to obtain the pre-tagged list of word/POS tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gore_bush_tagged_words = masc_tagged.tagged_words('spoken/2nd_Gore-Bush.txt')\n",
    "\n",
    "print(gore_bush_tagged_words[0:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call the method **tagged_sents()** to obtain the pre-tagged list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gore_bush_tagged_sents = masc_tagged.tagged_sents('spoken/2nd_Gore-Bush.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the list of word/POS tuples in the 100th sentence of the text, just index the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gore_bush_tagged_sents[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call the method **tagged_paras** to obtain the pre-tagged list of paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gore_bush_tagged_paras = masc_tagged.tagged_paras('spoken/2nd_Gore-Bush.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access tagged paragraph by index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gore_bush_tagged_paras[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the frequency distribution of each POS tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags1 = nltk.FreqDist(tag for (word, tag) in gore_bush_tagged_words)\n",
    "tags1.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a less in-depth analysis of POS tags, i.e. you don't care about person, number, or tense and just want to know noun, verb, adverb, etc., you can specify the tagset **universal** which has the following tags:\n",
    "\n",
    "- VERB - verbs (all tenses and modes)\n",
    "- NOUN - nouns (common and proper)\n",
    "- PRON - pronouns\n",
    "- ADJ - adjectives\n",
    "- ADV - adverbs\n",
    "- ADP - adpositions (prepositions and postpositions)\n",
    "- CONJ - conjunctions\n",
    "- DET - determiners\n",
    "- NUM - cardinal numbers\n",
    "- PRT - particles or other function words\n",
    "- X - other: foreign words, typos, abbreviations\n",
    "- . - punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gore_bush_universal_tagged_words = masc_tagged.tagged_words('spoken/2nd_Gore-Bush.txt', tagset='universal')\n",
    "tags2 = nltk.FreqDist(tag for (word, tag) in gore_bush_universal_tagged_words)\n",
    "tags2.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer/POS caution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the *Tokenizer* section of this tutorial, **word_tokenize()** is the NLTK recommended tokenizer, although we showed that the **RegexpTokenizer** produced the same top word count results.  But for different applications, these two different tokenizers will result in very different results.  A simple example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regExpTokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "text = \"I don't have shoes.\"\n",
    "\n",
    "regexp_tokens = regExpTokenizer.tokenize(text)\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Regexp toks: {}\".format(regexp_tokens))\n",
    "print(\"Word toks:   {}\".format(word_tokens))\n",
    "\n",
    "print(\"\\nRegexp tuples: {}\".format(nltk.pos_tag(regexp_tokens)))\n",
    "print(\"Word tuples:   {}\".format(nltk.pos_tag(word_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how even in a very simple sentence, the parts of speech vary.  The **word_tokenize()** method produces the tokenized output for *don't* as *do* and *n't*.  The **pos_tag()** method recognizes *n't* as the adverb *not*, whereas the output from the regular expression tokenizer is not recognized similarly and instead recognizes *don* as a different verb altogether than *do* and has no concept of negation.   To help analyze this final example further, look in detail at the help method for each POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"RB\")\n",
    "nltk.help.upenn_tagset(\"NNS\")\n",
    "nltk.help.upenn_tagset(\"VB\")\n",
    "nltk.help.upenn_tagset(\"VBP\")\n",
    "nltk.help.upenn_tagset(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading on NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://www.nltk.org/py-modindex.html The NLTK Python Module index\n",
    "* http://www.nltk.org/genindex.html The NLTK complete alphabetical index\n",
    "* https://github.com/nltk/nltk/wiki The NLTK WIKI\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
